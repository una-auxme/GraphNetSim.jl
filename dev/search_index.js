var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Configuration-and-Data-Structures","page":"API Reference","title":"Configuration and Data Structures","text":"","category":"section"},{"location":"api/#Args","page":"API Reference","title":"Args","text":"","category":"section"},{"location":"api/#Dataset","page":"API Reference","title":"Dataset","text":"","category":"section"},{"location":"api/#Dataset-Constructors","page":"API Reference","title":"Dataset Constructors","text":"","category":"section"},{"location":"api/#Training-and-Evaluation","page":"API Reference","title":"Training and Evaluation","text":"","category":"section"},{"location":"api/#Main-Training-Function","page":"API Reference","title":"Main Training Function","text":"","category":"section"},{"location":"api/#Internal-Training-Functions","page":"API Reference","title":"Internal Training Functions","text":"","category":"section"},{"location":"api/#Normalization-Setup","page":"API Reference","title":"Normalization Setup","text":"","category":"section"},{"location":"api/#Main-Evaluation-Function","page":"API Reference","title":"Main Evaluation Function","text":"","category":"section"},{"location":"api/#Training-Strategies","page":"API Reference","title":"Training Strategies","text":"","category":"section"},{"location":"api/#Abstract-Base-Type","page":"API Reference","title":"Abstract Base Type","text":"","category":"section"},{"location":"api/#Concrete-Strategies","page":"API Reference","title":"Concrete Strategies","text":"","category":"section"},{"location":"api/#Normalization-and-Data-Statistics","page":"API Reference","title":"Normalization and Data Statistics","text":"","category":"section"},{"location":"api/#Computing-Normalization-Statistics","page":"API Reference","title":"Computing Normalization Statistics","text":"","category":"section"},{"location":"api/#Graph-Construction-and-ODE-Solving","page":"API Reference","title":"Graph Construction and ODE Solving","text":"","category":"section"},{"location":"api/#Building-Computation-Graphs","page":"API Reference","title":"Building Computation Graphs","text":"","category":"section"},{"location":"api/#ODE-Integration","page":"API Reference","title":"ODE Integration","text":"","category":"section"},{"location":"api/#Data-Utilities-and-Conversion","page":"API Reference","title":"Data Utilities and Conversion","text":"","category":"section"},{"location":"api/#Dataset-Loading","page":"API Reference","title":"Dataset Loading","text":"","category":"section"},{"location":"api/#Format-Conversion","page":"API Reference","title":"Format Conversion","text":"","category":"section"},{"location":"api/#Visualization","page":"API Reference","title":"Visualization","text":"","category":"section"},{"location":"api/#VTK-Export","page":"API Reference","title":"VTK Export","text":"","category":"section"},{"location":"api/#GraphNetSim.Args","page":"API Reference","title":"GraphNetSim.Args","text":"Args\n\nConfiguration structure for training and evaluating Graph Neural Network simulators.\n\nFields\n\nNetwork Architecture\n\nmps::Integer=15: Number of message passing steps (higher = more expressive but slower)\nlayer_size::Integer=128: Latent dimension for MLP hidden layers\nhidden_layers::Integer=2: Number of hidden layers in each MLP module\n\nTraining Configuration\n\nepochs::Integer=1: Number of passes over the entire dataset\nsteps::Integer=10e6: Total number of training steps\ncheckpoint::Integer=10000: Interval (in steps) for saving checkpoints\nnorm_steps::Integer=1000: Steps to accumulate normalization statistics before weight updates\nbatchsize::Integer=1: Batch size (currently limited to 1 - full trajectory per batch)\n\nNormalization\n\nmax_norm_steps::Integer=10.0f6: Maximum steps for online normalizer accumulation\n\nData Augmentation\n\ntypes_updated::Vector{Integer}=[1]: Node types whose features are predicted\ntypes_noisy::Vector{Integer}=[0]: Node types to which noise is added during training\nnoise_stddevs::Vector{Float32}=[0.0f0]: Standard deviations for Gaussian noise (per type or broadcast)\n\nTraining Strategy\n\ntraining_strategy::TrainingStrategy=DerivativeTraining(): Method for computing loss\n\nHardware and Optimization\n\nuse_cuda::Bool=true: Enable CUDA GPU acceleration (if available)\ngpu_device::Union{Nothing,CuDevice}: CUDA device to use (auto-selected if CUDA functional)\noptimizer_learning_rate_start::Float32=1.0f-4: Initial learning rate\noptimizer_learning_rate_stop::Union{Nothing,Float32}=nothing: Final learning rate (for decay schedule)\n\nValidation\n\nshow_progress_bars::Bool=true: Show training progress bars\nuse_valid::Bool=true: Load validation checkpoint (best loss) instead of final checkpoint\nsolver_valid::OrdinaryDiffEqAlgorithm=Tsit5(): ODE solver for validation rollouts\nsolver_valid_dt::Union{Nothing,Float32}=nothing: Fixed timestep for validation solver\nreset_valid::Bool=false: Reset validation after loading checkpoint\nsave_step::Bool=false: Save loss at every step (can create large log files)\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.Dataset","page":"API Reference","title":"GraphNetSim.Dataset","text":"Dataset\n\nA mutable structure containing trajectory data and associated metadata for training, validation, or testing.\n\nFields\n\nmeta::Dict{String,Any}: Dictionary containing all metadata for the dataset, including feature names, trajectory information, and device settings.\ndatafile::String: Path to the data file (usually .h5 or .jld2 format).\nlock::ReentrantLock: Lock for thread-safe access to the datafile during concurrent operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.Dataset-Tuple{String, String, Any}","page":"API Reference","title":"GraphNetSim.Dataset","text":"Dataset(datafile::String, metafile::String, args)\n\nConstruct a Dataset from separate data and metadata files.\n\nValidates that both files exist and have correct formats (.h5 or .jld2 for data, .json for metadata), then loads trajectories and merges metadata with provided arguments.\n\nArguments\n\ndatafile::String: Path to the data file (.h5 or .jld2 containing trajectory data).\nmetafile::String: Path to the metadata file (.json containing dataset configuration).\nargs: A structure or object whose fields will be merged into the metadata dictionary.\n\nReturns\n\nDataset: A new Dataset object initialized with the provided data and metadata.\n\nThrows\n\nArgumentError: If datafile or metafile do not exist or have invalid formats.\n\n\n\n\n\n","category":"method"},{"location":"api/#GraphNetSim.Dataset-Tuple{Symbol, String, Any}","page":"API Reference","title":"GraphNetSim.Dataset","text":"Dataset(split::Symbol, path::String, args)\n\nConstruct a Dataset by specifying a split type and directory path.\n\nLocates and loads the appropriate data file based on the split type (:train, :valid, or :test), expecting a \"meta.json\" file in the given directory.\n\nArguments\n\nsplit::Symbol: The dataset split, one of :train, :valid, or :test.\npath::String: Directory path containing the metadata file \"meta.json\" and the corresponding data file.\nargs: A structure or object whose fields will be merged into the metadata dictionary.\n\nReturns\n\nDataset: A new Dataset object initialized with data from the specified split.\n\nThrows\n\nArgumentError: If split is invalid, if meta.json is not found, or if the corresponding data file cannot be found.\n\n\n\n\n\n","category":"method"},{"location":"api/#GraphNetSim.get_file","page":"API Reference","title":"GraphNetSim.get_file","text":"get_file(split::Symbol, path::String)\n\nLocate the data file corresponding to a dataset split in the given directory.\n\nAttempts to find a .jld2 file first, then a .h5 file with the split name (e.g., \"train.jld2\" or \"train.h5\").\n\nArguments\n\nsplit::Symbol: The dataset split name (converted to string).\npath::String: Directory path to search for the data file.\n\nReturns\n\nString: Full path to the located data file.\n\nThrows\n\nArgumentError: If no data file with the specified split name is found in the directory.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.train_network","page":"API Reference","title":"GraphNetSim.train_network","text":"train_network(opt, ds_path::String, cp_path::String; kws...)\n\nTrain a Graph Neural Network simulator on trajectory data.\n\nInitializes a graph network, loads dataset, computes normalization statistics,  and performs supervised training using the specified training strategy. Validates  periodically on validation set and saves checkpoints for the best model.\n\nArguments\n\nopt: Optimizer configuration (e.g., Optimisers.Adam(1f-4)).\nds_path::String: Path to dataset directory (must contain train, valid, test splits).\ncp_path::String: Path where checkpoints and logs are saved.\n\nKeyword Arguments\n\nmps::Int=15: Number of message passing steps in the network.\nlayer_size::Int=128: Latent dimension of hidden MLP layers.\nhidden_layers::Int=2: Number of hidden layers in each MLP.\nbatchsize::Int=1: Batch size for training (default uses full trajectories).\nepochs::Int=1: Number of training epochs.\nsteps::Int=10e6: Total number of training steps.\ncheckpoint::Int=10000: Create checkpoint every N steps.\nnorm_steps::Int=1000: Steps for accumulating normalization statistics without updates.\nmax_norm_steps::Float32=10.0f6: Maximum steps for online normalizers.\ntypes_updated::Vector{Int}=[1]: Node types whose features are predicted.\ntypes_noisy::Vector{Int}=[0]: Node types to which noise is added.\nnoise_stddevs::Vector{Float32}=[0.0f0]: Standard deviations for Gaussian noise.\ntraining_strategy::TrainingStrategy=DerivativeTraining(): Training method to use.\nuse_cuda::Bool=true: Use CUDA GPU if available.\nsolver_valid::OrdinaryDiffEqAlgorithm=Tsit5(): ODE solver for validation rollouts.\nsolver_valid_dt::Union{Nothing,Float32}=nothing: Fixed timestep for validation (if set).\noptimizer_learning_rate_start::Float32=1.0f-4: Initial learning rate.\noptimizer_learning_rate_stop::Union{Nothing,Float32}=nothing: Final learning rate for decay schedule.\nshow_progress_bars::Bool=true: Show training progress bars.\nuse_valid::Bool=true: Use validation checkpoint for early stopping.\n\nReturns\n\nFloat32: Minimum validation loss achieved during training.\n\nTraining Strategies\n\nDerivativeTraining: Train on current step derivatives (collocation)\nBatchingStrategy: Custom batching of trajectory segments\n\nExample #TODO add example with different training strategies\n\ntrain_network(\n    Optimisers.Adam(1f-4),\n    \"./data\",\n    \"./checkpoints\";\n    epochs=10,\n    steps=50000,\n    mps=15,\n    layer_size=128,\n    training_strategy=DerivativeTraining()\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.train_gns!","page":"API Reference","title":"GraphNetSim.train_gns!","text":"train_gns!(gns::GraphNetwork, opt_state, ds_train::Dataset, ds_valid::Dataset, df_train, df_valid, df_step, device::Function, cp_path::String, args::Args)\n\nExecute the main training loop for a Graph Neural Network simulator.\n\nPerforms supervised learning with periodic validation, checkpoint saving, and learning rate scheduling. Supports various training strategies (derivative, batching) and handles feature noise injection, gradient accumulation over multiple time steps, and online normalizer updates.\n\nArguments\n\ngns::GraphNetwork: Graph network model containing parameters and normalizers.\nopt_state: Optimizer state from Optimisers.jl.\nds_train::Dataset: Training dataset with trajectories and metadata.\nds_valid::Dataset: Validation dataset for monitoring training progress.\ndf_train: DataFrame storing training loss at checkpoints.\ndf_valid: DataFrame storing best validation losses.\ndf_step: DataFrame storing loss at each step (if save_step enabled).\ndevice::Function: Device placement function (cpudevice or gpudevice).\ncp_path::String: Path for saving checkpoints and logs.\nargs::Args: Configuration including optimizer, strategy, and training parameters.\n\nAlgorithm\n\nFor each training step:\n\nPrepare input data and compute node features\nFor each time step in trajectory:\nBuild computation graph with neighborhood connections\nForward pass through network\nCompute loss depending on training strategy\nAccumulate gradients\nUpdate parameters after accumulation window\nOptionally decay learning rate\nEvery N steps: validate on full trajectories and save checkpoint if improved\n\nReturns\n\nFloat32: Minimum validation loss achieved.\n\nNotes\n\nFirst norm_steps steps only accumulate normalization statistics\nValidation using long trajectory rollouts occurs at checkpoint intervals\nCheckpoints saved to cp_path/valid when validation loss improves\nFinal checkpoint always saved at cp_path\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.calc_norms","page":"API Reference","title":"GraphNetSim.calc_norms","text":"calc_norms(dataset::Dataset, device::Function, args::Args)\n\nInitialize and compute feature normalizers from dataset statistics.\n\nComputes normalization statistics for edge features, node features, and output features based on metadata specifications. Supports offline min/max and mean/std normalization, boolean encoding, one-hot encoded features, and online accumulation strategies.\n\nArguments\n\ndataset::Dataset: Dataset object containing feature metadata and specifications.\ndevice::Function: Device placement function (cpudevice or gpudevice).\nargs::Args: Configuration struct with norm_steps for online normalizer accumulation.\n\nReturns\n\nTuple: (quantities, enorms, nnorms, o_norms)\nquantities::Int: Total number of input feature dimensions\ne_norms: Dictionary or single normalizer for edge features\nn_norms::Dict: Dictionary mapping node feature names to normalizers\no_norms::Dict: Dictionary mapping output feature names to normalizers\n\nNormalizer Types\n\nNormaliserOfflineMinMax: Fixed min/max normalization with learnable target range\nNormaliserOfflineMeanStd: Fixed mean/standard deviation normalization\nNormaliserOnline: Accumulates statistics online during training\n\nNotes\n\nOne-hot encoded Int32 features are expanded to multiple dimensions\nBoolean features are mapped to [0.0, 1.0] range\nDistance features add dimensions for domain boundary constraints\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.eval_network","page":"API Reference","title":"GraphNetSim.eval_network","text":"eval_network(ds_path::String, cp_path::String, out_path::String, solver=nothing; start, stop, dt=nothing, saves, mse_steps, kws...)\n\nEvaluate a trained Graph Neural Network simulator on test trajectories.\n\nLoads a trained network from checkpoint, performs long-term trajectory rollouts, computes error metrics against ground truth, and saves results to disk.\n\nArguments\n\nds_path::String: Path to dataset directory containing test split.\ncp_path::String: Path to checkpoint directory (contains model parameters).\nout_path::String: Path where evaluation results are saved.\nsolver: ODE solver for long-term predictions (e.g., Tsit5()).\n\nKeyword Arguments\n\nstart::Real: Start time for evaluation.\nstop::Real: End time for evaluation.\nsaves::AbstractVector: Time points where solution is saved.\nmse_steps::AbstractVector: Time points where error metrics are computed.\ndt::Union{Nothing,Real}=nothing: Fixed timestep for solver (if applicable).\nmps::Int=15: Number of message passing steps (must match training config).\nlayer_size::Int=128: Hidden layer size (must match training config).\nhidden_layers::Int=2: Number of hidden layers (must match training config).\ntypes_updated::Vector{Int}=[1]: Updated node types (must match training config).\nuse_cuda::Bool=true: Use CUDA GPU if available.\nuse_valid::Bool=true: Load from best validation checkpoint instead of final checkpoint.\n\nOutput\n\nSaves results to out_path/{solver_name}/trajectories.h5:\n\nGround truth positions, velocities, accelerations\nPredicted positions, velocities, accelerations\nPrediction errors for each trajectory\n\nExample\n\neval_network(\n    \"./data\",\n    \"./checkpoints\",\n    \"./results\";\n    solver=Tsit5(),\n    start=0.0f0,\n    stop=1.0f0,\n    dt=0.01f0,\n    saves=0.0:0.01:1.0,\n    mse_steps=0.0:0.1:1.0\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.eval_network!","page":"API Reference","title":"GraphNetSim.eval_network!","text":"eval_network!(solver, gns::GraphNetwork, ds_test::Dataset, device::Function, out_path::String, start::Real, stop::Real, dt, saves, mse_steps, args::Args)\n\nPerform evaluation loops and trajectory rollouts for all test samples.\n\nExecutes long-term predictions for each test trajectory, computes performance metrics relative to ground truth, and saves trajectories and errors to HDF5 format.\n\nArguments\n\nsolver: ODE solver for trajectory rollouts (or nothing for collocation).\ngns::GraphNetwork: Trained graph network model.\nds_test::Dataset: Test dataset with trajectories.\ndevice::Function: Device placement function.\nout_path::String: Output directory for results.\nstart::Real: Evaluation start time.\nstop::Real: Evaluation end time.\ndt: Fixed timestep (or nothing for adaptive).\nsaves: Time points to save solution.\nmse_steps: Time points to compute errors.\nargs::Args: Configuration parameters.\n\nAlgorithm\n\nFor each test trajectory:\n\nExtract initial conditions from data\nCreate computation graph with graph network\nRoll out trajectory using ODE solver for specified duration\nExtract position, velocity, and acceleration from solution\nCompute mean squared error against ground truth\nReport cumulative error at specified time points\n\nReturns\n\nTuple: (traj_ops, errors)\ntraj_ops::Dict: Dictionary of trajectories with ground truth and predictions\nerrors::Dict: Squared errors for each trajectory\n\nOutput Files\n\nCreates {out_path}/{solver_name}/trajectories.h5 containing:\n\nGround truth and predicted trajectories\nError fields for each time step\nProperly indexed for easy post-processing\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.prepare_training","page":"API Reference","title":"GraphNetSim.prepare_training","text":"prepare_training(strategy)\n\nFunction that is executed once before training. Can be overwritten by training strategies if necessary.\n\nArguments\n\nstrategy: Used training strategy.\n\nReturns\n\nTuple containing the results of the function.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.get_delta","page":"API Reference","title":"GraphNetSim.get_delta","text":"get_delta(strategy, trajectory_length)\n\nReturns the delta between samples in the training data.\n\nArguments\n\nstrategy: Used training strategy.\nTrajectory length (used for Derivative strategies).\n\nReturns\n\nDelta between samples in the training data.\n\n\n\n\n\nget_delta(::SolverStrategy, ::Integer)\n\nReturns the delta (step size) for solver-based training strategies.\n\nFor most solver-based strategies, returns 1 (advancing by single timestep). Can be overridden by specific strategies (e.g., BatchingStrategy).\n\nArguments\n\nstrategy::SolverStrategy: Solver-based training strategy.\ntrajectory_length::Integer: Length of the trajectory (unused in base implementation).\n\nReturns\n\nInteger delta between training samples.\n\n\n\n\n\nget_delta(strategy::BatchingStrategy, ::Integer)\n\nReturns the number of steps per batch.\n\nArguments\n\nstrategy::BatchingStrategy: BatchingStrategy instance.\nUnused trajectory length parameter.\n\nReturns\n\nInteger: Number of steps per batch (strategy.steps).\n\n\n\n\n\nget_delta(strategy::DerivativeStrategy, trajectory_length)\n\nReturns the effective trajectory length for derivative training.\n\nIf strategy windowsize > 0 and smaller than trajectorylength, returns windowsize. Otherwise returns the full trajectorylength.\n\nArguments\n\nstrategy::DerivativeStrategy: Derivative-based strategy.\ntrajectory_length::Integer: Length of the trajectory.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.init_train_step","page":"API Reference","title":"GraphNetSim.init_train_step","text":"init_train_step(strategy, t)\n\nFunction that is executed before each training sample.\n\nArguments\n\nstrategy: Used training strategy.\nt: Tuple containing the variables necessary for initializing training.\nta: Tuple with additional variables that is returned from prepare_training.\n\nReturns\n\nTuple containing variables needed for train_step.\n\n\n\n\n\ninit_train_step(strategy::SolverStrategy, t::Tuple)\n\nInitializes a training step for solver-based strategies.\n\nExtracts initial conditions, packs state into ComponentArray format, and prepares ground truth data for ODE problem setup.\n\nArguments\n\nstrategy::SolverStrategy: Solver-based training strategy.\nt::Tuple: Input tuple containing (gns, data, position, velocity, meta, outputfields, targetfields, node_type, mask, device, ...).\n\nReturns\n\nTuple: Initialized data for training step.\n\n\n\n\n\ninit_train_step(strategy::BatchingStrategy, t::Tuple)\n\nInitializes a training step for the BatchingStrategy.\n\nSelects the next batch via nextBatch(), extracts initial conditions for that time window, packs state into ComponentArray, and prepares ground truth data.\n\nArguments\n\nstrategy::BatchingStrategy: BatchingStrategy instance.\nt::Tuple: Input tuple (gns, data, meta, outputfields, targetfields, nodetype, mask, valmask, device, , batches, showprogress_bars).\n\nReturns\n\nTuple: Initialized batch data for training step.\n\n\n\n\n\ninit_train_step(strategy::DerivativeStrategy, t::Tuple)\n\nInitializes a training step for derivative-based strategies.\n\nExtracts target derivatives at a single datapoint and normalizes using network feature normalizers.\n\nArguments\n\nstrategy::DerivativeStrategy: Derivative-based strategy.\nt::Tuple: Input tuple with network, data, and sampling information.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.train_step","page":"API Reference","title":"GraphNetSim.train_step","text":"train_step(strategy, t)\n\nPerforms a single training step and return the resulting gradients and loss.\n\nArguments\n\nstrategy: Solver strategy that is used for training.\nt: Tuple that is returned from init_train_step.\n\nReturns\n\nGradients for optimization step.\nLoss for optimization step.\n\n\n\n\n\ntrain_step(strategy::SolverStrategy, t::Tuple)\n\nPerforms one training step for solver-based strategies.\n\nConstructs an ODE problem from the GNS model, solves it using the strategy's solver, and computes gradients via sensitivity analysis (adjoint method).\n\nArguments\n\nstrategy::SolverStrategy: Solver-based training strategy.\nt::Tuple: Initialized data from init_train_step().\n\nReturns\n\nTuple: (gradients, loss) - Gradients for optimization and scalar training loss.\n\nAlgorithm\n\nCreate ODE right-hand side function using ode_func_train().\nSetup ODE problem with initial conditions and parameters.\nCompute loss via train_loss().\nBackpropagate through ODE solver using sensitivity algorithm.\nReturn gradients and loss.\n\n\n\n\n\ntrain_step(strategy::BatchingStrategy, t::Tuple)\n\nPerforms one training step for the BatchingStrategy.\n\nConstructs an ODE problem for the selected batch, solves it using the strategy's solver, and computes gradients via sensitivity analysis. Updates batch loss.\n\nArguments\n\nstrategy::BatchingStrategy: BatchingStrategy instance.\nt::Tuple: Data tuple from inittrainstep().\n\n\n\n\n\ntrain_step(strategy::DerivativeStrategy, t::Tuple)\n\nPerforms one training step for derivative-based strategies.\n\nEvaluates network on graph at a single timepoint and computes loss against target derivatives. Computes gradients via backpropagation.\n\nArguments\n\nstrategy::DerivativeStrategy: Derivative-based strategy.\nt::Tuple: Data tuple from inittrainstep().\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.validation_step","page":"API Reference","title":"GraphNetSim.validation_step","text":"validation_step(strategy, t)\n\nPerforms validation of a single trajectory. Should be overwritten by training strategies to determine simulation and data interval before calling the inner function _validation_step.\n\nArguments\n\nstrategy: Type of training strategy (used for dispatch).\nt: Tuple containing the variables necessary for validation.\n\nReturns\n\nSee _validation_step.\n\n\n\n\n\nvalidation_step(strategy::SolverStrategy, t::Tuple)\n\nValidation step for solver-based strategies.\n\nComputes validation loss by rolling out the GNS model over the full validation trajectory and comparing predicted outputs with ground truth.\n\nArguments\n\nstrategy::SolverStrategy: Solver-based training strategy.\nt::Tuple: Validation data tuple containing (gns, data, meta, ...).\n\nReturns\n\nFloat32: Validation loss (mean squared error).\n\n\n\n\n\nvalidation_step(strategy::DerivativeStrategy, t::Tuple)\n\nValidation step for derivative-based strategies.\n\nComputes validation loss by rolling out GNS model over trajectory window and comparing derivatives with ground truth.\n\nArguments\n\nstrategy::DerivativeStrategy: Derivative-based strategy.\nt::Tuple: Validation data tuple.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim._validation_step","page":"API Reference","title":"GraphNetSim._validation_step","text":"_validation_step(t, sim_interval, data_interval)\n\nInner function for validation of a single trajectory.\n\nArguments\n\nt: Tuple containing the variables necessary for validation.\nsim_interval: Interval that determines the simulated time for the validation.\ndata_interval: Interval that determines the indices of the timesteps in ground truth and prediction data.\n\nReturns\n\nLoss calculated on the difference between ground truth and prediction (via mse).\nGround truth data with data_interval as timesteps.\nPrediction data with data_interval as timesteps.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.batchTrajectory","page":"API Reference","title":"GraphNetSim.batchTrajectory","text":"batchTrajectory(strategy::BatchingStrategy, data::Dict)\n\nPartitions a trajectory into time intervals (batches) for sequential training.\n\nDivides the full trajectory duration into equal-sized time intervals, creating one Batch object per interval. Used for memory-efficient training on long sequences.\n\nArguments\n\nstrategy::BatchingStrategy: Batching strategy with interval specifications.\ndata::Dict: Data dictionary containing \"dt\" (timestep) and \"trajectory_length\".\n\nReturns\n\nVector{Batch}: Array of Batch objects partitioning the trajectory.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.SingleShooting","page":"API Reference","title":"GraphNetSim.SingleShooting","text":"SingleShooting(tstart, dt, tstop, solver; sense = InterpolatingAdjoint(autojacvec = ZygoteVJP()), solargs...)\n\nThe default solver based training that is normally used for NeuralODEs. Simulates the system from tstart to tstop and calculates the loss based on the difference between the prediction and the ground truth at the timesteps tstart:dt:tstop.\n\nArguments\n\ntstart: Start time of the simulation.\ndt: Interval at which the simulation is saved.\ntstop: Stop time of the simulation.\nsolver: Solver that is used for simulating the system.\n\nKeyword Arguments\n\nsense = InterpolatingAdjoint(autojacvec = ZygoteVJP()): The sensitivity algorithm that is used for caluclating the sensitivities.\nsolargs: Keyword arguments that are passed on to the solver.\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.MultipleShooting","page":"API Reference","title":"GraphNetSim.MultipleShooting","text":"MultipleShooting(tstart, dt, tstop, solver, interval_size, continuity_term = 100; sense = InterpolatingAdjoint(autojacvec = ZygoteVJP(), checkpointing = true), solargs...)\n\nSimilar to SingleShooting, but splits the trajectory into intervals that are solved independently and then combines them for loss calculation. Useful if the network tends to get stuck in a local minimum if SingleShooting is used.\n\nArguments\n\ntstart: Start time of the simulation.\ndt: Interval at which the simulation is saved.\ntstop: Stop time of the simulation.\nsolver: Solver that is used for simulating the system.\ninterval_size: Size of the intervals (i.e. number of datapoints in one interval).\ncontinuity_term = 100: Factor by which the error between points of concurrent intervals is multiplied.\n\nKeyword Arguments\n\nsense = InterpolatingAdjoint(autojacvec = ZygoteVJP(), checkpointing = true):\nsolargs: Keyword arguments that are passed on to the solver.\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.DerivativeTraining","page":"API Reference","title":"GraphNetSim.DerivativeTraining","text":"struct DerivativeTraining <: DerivativeStrategy\n\nDerivative-based training strategy using finite-difference ground truth.\n\nCompares network output with finite-difference derivatives from data. Faster than solver-based training, useful for initial model training. Supports temporal windowing and optional random shuffling.\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.BatchingStrategy","page":"API Reference","title":"GraphNetSim.BatchingStrategy","text":"struct BatchingStrategy <: SolverStrategy\n\nSolver-based training strategy that batches long trajectories into segments.\n\nDivides long trajectories into time intervals and solves/trains on each segment independently. Useful for memory-efficient training on long sequences.\n\n\n\n\n\n","category":"type"},{"location":"api/#GraphNetSim.data_minmax","page":"API Reference","title":"GraphNetSim.data_minmax","text":"data_minmax(path)\n\nCalculates the minimum and maximum values for each numeric feature across all dataset partitions.\n\nIterates through training, validation, and test datasets to compute global min/max bounds for all numeric features (Int32 and Float32 types).\n\nArguments\n\npath: Path to the dataset files.\n\nReturns\n\nDict: Dictionary mapping feature names to [min, max] value pairs computed from all datasets.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.data_meanstd","page":"API Reference","title":"GraphNetSim.data_meanstd","text":"data_meanstd(path)\n\nCalculates the mean and standard deviation for each feature in the given part of the dataset.\n\nArguments\n\npath: Path to the dataset files.\n\nReturns\n\nMean and standard deviation in training, validation and test set\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.der_minmax","page":"API Reference","title":"GraphNetSim.der_minmax","text":"der_minmax(path)\n\nCalculates the minimum and maximum across training, validation, and test sets for each numeric feature.\n\nCombines results from both training/validation and test data to compute overall min/max bounds.\n\nArguments\n\npath: Path to the dataset files.\n\nReturns\n\nDict: Dictionary mapping feature names to [min, max] value pairs across all datasets.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.build_graph","page":"API Reference","title":"GraphNetSim.build_graph","text":"build_graph(gns::GraphNetCore.GraphNetwork, data::Dict{String,Any}, datapoint::Integer, meta, node_type, device)\n\nConstruct a FeatureGraph from trajectory data at a specific time step.\n\nExtracts position and velocity data from the trajectory dictionary at the given time point, then delegates to the second method to construct the graph with edge connectivity and normalized features.\n\nArguments\n\ngns::GraphNetCore.GraphNetwork: Graph network model containing normalizers for features.\ndata::Dict{String,Any}: Dictionary containing trajectory data (position, velocity, etc.).\ndatapoint::Integer: Time step index to extract from the trajectory.\nmeta::Dict{String,Any}: Metadata dictionary with connectivity and feature settings.\nnode_type: One-hot encoded node type features.\ndevice::Function: Device placement function (cpu or gpu).\n\nReturns\n\nGraphNetCore.FeatureGraph: Constructed graph with normalized node and edge features.\n\n\n\n\n\nbuild_graph(gns::GraphNetCore.GraphNetwork, position, velocity, meta, node_type, mask, device)\n\nConstruct a FeatureGraph from position and velocity data with edge connectivity.\n\nComputes edges based on spatial proximity using GPU-accelerated neighborhood search, calculates relative displacements and normalized distances. Node features are constructed from position, velocity, node type, and distance bounds to domain boundaries. All features are normalized using the normalizers stored in the model.\n\nArguments\n\ngns::GraphNetCore.GraphNetwork: Graph network model containing normalizers.\nposition::AbstractArray: Particle positions with shape (dims, n_particles).\nvelocity::AbstractArray: Particle velocities with shape (dims, n_particles).\nmeta::Dict{String,Any}: Metadata with defaultconnectivityradius, bounds, dims, input_features, and device settings.\nnode_type::AbstractArray: One-hot encoded node type features.\nmask::AbstractVector: Indices of particles to include in the graph (fluid particles).\ndevice::Function: Device placement function.\n\nReturns\n\nGraphNetCore.FeatureGraph: Graph with normalized node features, normalized edge features, sender and receiver indices.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.rollout","page":"API Reference","title":"GraphNetSim.rollout","text":"rollout(solver, gns::GraphNetwork, initial_state, output_fields, meta, target_fields, node_type, mask, val_mask, start, stop, dt, saves, device; pr=nothing)\n\nSolves the ODE problem for a Graph Neural Network simulator using the given solver and computes solution at specified timesteps.\n\nSolves the ODEProblem of the GNS model over the specified time interval. The function handles both fixed and adaptive timestep solvers, with optional progress reporting.\n\nArguments\n\nsolver: ODE solver algorithm (e.g., Tsit5(), RK4()) from OrdinaryDiffEq.jl.\ngns::GraphNetwork: Graph neural network model to evaluate for dynamics.\ninitial_state::Dict: Dictionary with \"position\" and \"velocity\" arrays for initial conditions.\noutput_fields::Vector{String}: Names of output features predicted by the network.\nmeta::Dict: Dataset metadata containing feature dimensions and specifications.\ntarget_fields::Vector{String}: Names of target (output) features.\nnode_type::Vector: One-hot encoded node type indicators.\nmask::Vector: Boolean mask for valid nodes in graph.\nval_mask::Vector: Validation/evaluation mask for output features.\nstart::Float32: Start time of ODE integration.\nstop::Float32: Stop time of ODE integration.\ndt::Union{Nothing,Float32}: Fixed timestep (if nothing, uses adaptive timestepping).\nsaves::Vector: Timesteps where solution should be saved.\ndevice::Function: Device placement function (cpudevice or gpudevice).\n\nKeyword Arguments\n\npr::Union{Nothing,ProgressBar}=nothing: Progress bar for tracking ODE solve.\n\nReturns\n\nsol: Solution object containing state trajectories at specified saves timesteps.\n\nNotes\n\nUses ode_step_eval() as the right-hand side function for the ODE.\nState is packed as ComponentArray with x (position) and dx (velocity) fields.\nOutput features are denormalized using stored normalizers before return.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.keystraj","page":"API Reference","title":"GraphNetSim.keystraj","text":"keystraj(datafile::String)\n\nExtract trajectory keys from a data file.\n\nOpens either a .jld2 or .h5 file and returns all top-level keys, representing individual trajectories stored in the file.\n\nArguments\n\ndatafile::String: Path to the data file (.h5 or .jld2).\n\nReturns\n\nArray{String,1}: Array of trajectory keys from the file.\n\n\n\n\n\n","category":"function"},{"location":"api/#MLCore.getobs!","page":"API Reference","title":"MLCore.getobs!","text":"MLUtils.getobs!(buffer::Dict{String,Any}, ds::Dataset, idx::Int)\n\nLoad trajectory data into a pre-allocated buffer using the MLUtils interface.\n\nRetrieves a single trajectory by index, populates all metadata and features into the provided buffer dictionary, and applies trajectory preparation (device transfer, masking, validation masks). Modifies the buffer in-place.\n\nArguments\n\nbuffer::Dict{String,Any}: Pre-allocated dictionary to store trajectory data (modified in-place).\nds::Dataset: The dataset object.\nidx::Int: Index of the trajectory to retrieve (1-indexed).\n\nReturns\n\nDict{String,Any}: The modified buffer containing the trajectory data.\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.csv_to_hdf5","page":"API Reference","title":"GraphNetSim.csv_to_hdf5","text":"csv_to_hdf5(source::String, output::String; \n             dt::Float64=0.01, \n             n_trajectories::Int=1,\n             dims::Vector{Int}=[1, 2],\n             groupby_col::Symbol=:Idp,\n             interpolation_scheme::String=\"pchip\",\n             pos_col_prefix::String=\"Points\",\n             vel_col_prefix::String=\"Vel\",\n             type_col::Symbol=:Type,\n             extra_fields::Vector{Symbol}=Symbol[])\n\nConvert particle trajectory data from CSV to HDF5 format with computed accelerations.\n\nArguments\n\nsource::String: Path to input CSV file\noutput::String: Path to output HDF5 file\n\nKeyword Arguments\n\ndt::Float64: Time step (default: 0.01)\nn_trajectories::Int: Number of trajectories to process (default: 1)\ndims::Vector{Int}: Spatial dimensions to extract, e.g. [1, 2] or [1, 3].(default: [1, 2])\ngroupby_col::Symbol: Column name for grouping particles (default: :Idp)\ninterpolation_scheme::String: Acceleration calculation method (default: \"pchip\")\n\"central_diff\": Central difference scheme\n\"forward_diff\": Forward difference scheme\n\"backward_diff\": Backward difference scheme\n\"from_pos\": Acceleration from position (2nd order central difference)\n\"pchip\": PCHIP interpolation of velocity derivatives\n\"linear\": LinearInterpolation\n\"quadratic\": QuadraticInterpolation\n\"cubic_spline\": CubicSpline\n\"quadratic_spline\": QuadraticSpline\n\"cubic_hermite\": CubicHermiteSpline\n\"lagrange\": LagrangeInterpolation\n\"akima\": AkimaInterpolation\npos_col_prefix::String: Prefix for position columns (default: \"Points\")\nvel_col_prefix::String: Prefix for velocity columns (default: \"Vel\")\ntype_col::Symbol: Column name for particle type (default: :Type)\nextra_fields::Vector{Symbol}: Additional CSV columns to copy to HDF5  (e.g. [:Mass, :Temperature])\n\nExample\n\n# 3D simulation with PCHIP interpolation\ncsv_to_hdf5(\"data/dam_break.csv\", \"data/dam_break_first.h5\"; \n            dt=0.01, dims=[1, 2, 3], interpolation_scheme=\"pchip\")\n\n# 2D (skip y-dimension), with extra fields\ncsv_to_hdf5(\"data/input.csv\", \"output.h5\"; \n            dims=[1, 3], interpolation_scheme=\"cubic_spline\",\n            extra_fields=[:Mass, :Pressure])\n\n\n\n\n\n","category":"function"},{"location":"api/#GraphNetSim.visualize","page":"API Reference","title":"GraphNetSim.visualize","text":"visualize(inPath, outFolder, Position, subgroupTrajectory, Parameters, Trajectorys, NumberOfTimesteps)\n\nReads HDF5 file into dictionary and writes VTK HDF5 file format with dictToVTKHDF().\n\nInput file must have a group for each trajectory and subgroups for all trajectories used. Datasets are linked to each subgroup of trajectory. If only certain trajectories should be written, specify them with a Vector{Int} (minimum is 1). Validation contains the names of the subgroups which will be written and each trajectory must have the same number of timesteps.\n\nTimesteps can be automatically detected if dataset \"timesteps\" is linked to each trajectory group. This dataset must contain the number for the largest timestep and timesteps 1:1:max will be read. When using timesteps, all datasets to be read need \"[Int]\" appended.\n\nArguments\n\ninPath::String: Complete path ending with .h5 file.\noutFolder::String: Complete path for output folder.\nPosition::String: Name of HDF5 dataset for position data.\nsubgroupTrajectory::String: Name of subgroup within trajectory groups.\nParameters::Vector{String}: Names of other HDF5 datasets to read (optional).\nTrajectorys::Union{Vector{Int},Nothing}: Trajectory indices to write (optional, auto-detected if nothing).\nNumberOfTimesteps::Union{Vector{Int},Nothing}: Timesteps to write (optional, auto-detected if nothing).\n\nReturns\n\nDict: Dictionary mapping (trajectory, dataset_name, timestep) tuples to numerical arrays.\n\n\n\n\n\n","category":"function"},{"location":"#GraphNetSim.jl","page":"Home","title":"GraphNetSim.jl","text":"(Image: Docs) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: Code Style: Blue)","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"GraphNetSim.jl is a Julia package for training and evaluating Graph Neural Network (GNN) simulators for learning complex physical dynamics from trajectory data. It provides a comprehensive framework for:\n\nGraph-based learning of particle dynamics and fluid mechanics\nFlexible training strategies including batching and derivative-based approaches\nEfficient GPU acceleration via CUDA with automatic device management\nAutomatic normalization with offline (min-max, mean-std) and online strategies\nLong-term trajectory rollouts using ODE solvers for generalization assessment\nFeature noise injection for robust model training\n\nThe package is build upon GraphNetCore.jl for the underlying graph neural network architecture.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"To add GraphNetSim.jl to your Julia environment, use:\n\nusing Pkg\nPkg.add(\"GraphNetSim\")\n\nOr in the Julia REPL, press ] to enter package mode and type:\n\npkg> add GraphNetSim","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"Here's a minimal example of training a Graph Neural Network simulator on trajectory data:\n\nusing GraphNetSim\nusing Optimisers  # For optimization\nusing OrdinaryDiffEq  # For ODE solvers\n\n# Path to your dataset containing train/valid/test splits\nds_path = \"./path/to/dataset\"\n\n# Directory where checkpoints will be saved\ncp_path = \"./checkpoints\"\n\n# Train the network with default configuration\nmin_loss = train_network(\n    Optimisers.Adam(1.0f-4),  # Optimizer and learning rate\n    ds_path,\n    cp_path;\n    epochs=10,\n    steps=50000,\n    mps=15,                    # Message passing steps\n    layer_size=128,            # Hidden layer dimension\n    hidden_layers=2,           # Number of hidden layers per MLP module\n    checkpoint=1000,           # Save checkpoint every N steps\n    use_cuda=true,             # Enable GPU acceleration if available\n    training_strategy=DerivativeTraining()  # Training strategy\n)\n\n# Evaluate the trained network with long-term rollouts\neval_network(\n    ds_path,\n    cp_path,\n    \"./results\";               # Output directory\n    solver=Tsit5(),            # ODE solver\n    start=0.0f0,\n    stop=1.0f0,\n    saves=0.0:0.01:1.0,        # Time points to save\n    mse_steps=0.0:0.1:1.0      # Time points for error metrics\n)\n\nFurther examples will be added soon.","category":"section"},{"location":"#Dataset-Format","page":"Home","title":"Dataset Format","text":"Datasets should be organized as:\n\ndataset/\n├── meta.json           # Metadata (feature specs, topology, etc.)\n├── train.h5            # Training trajectories\n├── valid.h5            # Validation trajectories\n└── test.h5             # Test trajectories\n\nThe metadata file defines feature dimensions, node types, graph connectivity, and normalization settings.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Multiple Training Strategies: Choose between BatchingTraining and DerivativeTraining to suit your problem\nGPU-accelerated Training: Automatic CUDA detection and memory management\nFlexible Architecture: Configurable message passing steps, layer sizes, and hidden layers\nProgress Monitoring: Built-in progress bars and logging for training and validation","category":"section"},{"location":"#Architecture-Overview","page":"Home","title":"Architecture Overview","text":"The package implements a graph-based approach to physics simulation:\n\nGraph Construction: Particles and boundaries are represented as nodes; interactions are represented as edges based on spatial proximity\nMessage Passing: Graph neural networks aggregate information from neighboring particles through multiple message passing iterations\nDynamics Prediction: The network predicts accelerations, velocities, or other output features based on learned interactions\nODE Integration: Predicted dynamics are integrated forward in time using standard ODE solvers","category":"section"},{"location":"#Configuration","page":"Home","title":"Configuration","text":"Training is controlled via the Args structure, which accepts keyword arguments:\n\nargs = GraphNetSim.Args(\n    mps=15,                                  # Message passing steps\n    layer_size=128,                          # Hidden dimension\n    hidden_layers=2,                         # Layers per MLP\n    epochs=1,                                # Training epochs\n    steps=10e6,                              # Total steps\n    checkpoint=10000,                        # Checkpoint interval\n    norm_steps=1000,                         # Online norm accumulation\n    types_updated=[1],                       # Updated node types\n    types_noisy=[0],                         # Noisy node types\n    noise_stddevs=[0.0f0],                   # Noise levels\n    training_strategy=DerivativeTraining(),  # Training strategy\n    use_cuda=true,                           # GPU acceleration\n    optimizer_learning_rate_start=1.0f-4,    # Initial learning rate\n    optimizer_learning_rate_stop=nothing,    # Final learning rate (for scheduling)\n    use_valid=true,                          # Use best validation checkpoint\n    show_progress_bars=true                  # Show progress\n)\n\nSee the full API documentation for complete parameter details.","category":"section"},{"location":"#Visualization","page":"Home","title":"Visualization","text":"Export predicted trajectories as VTK files for visualization:\n\nvisualize(\n    \"trajectories.h5\",           # Results file from eval_network\n    \"./vtk_output\",              # Output directory\n    \"pos\",                        # Position dataset name\n    \"prediction\";                 # Subgroup to visualize\n    Trajectorys=1:5              # Trajectory indices\n)","category":"section"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"PointNeighbors.jl: Efficient spatial indexing for neighbor queries\nGraphNetCore.jl: Core GNN architecture and normalization strategies\nDifferentialEquations.jl: ODE solvers for trajectory integration","category":"section"},{"location":"#References","page":"Home","title":"References","text":"This package is inspired by the Graph Network-based Simulator (GNS) framework:\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., et al. (2020). \"Learning to Simulate Complex Physics with Graph Networks.\" Proceedings of the 37th International Conference on Machine Learning (ICML).","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"We welcome contributions to GraphNetSim.jl! Please follow the ColPrac guidelines for collaborative practices.","category":"section"}]
}
